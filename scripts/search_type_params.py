"""
Typeç‰¹å®šå‚æ•°(gamma_0)è‡ªåŠ¨æœç´¢è„šæœ¬

ä½¿ç”¨ä¸¤é˜¶æ®µç­–ç•¥å¯»æ‰¾æœ€ä¼˜çš„typeè¿ç§»æˆæœ¬å‚æ•°ï¼š
1. ç²—ç²’åº¦ç½‘æ ¼æœç´¢ï¼šå¿«é€Ÿè¯„ä¼°å¤§é‡å‚æ•°ç»„åˆ
2. ç²¾ç»†åŒ–ä¼˜åŒ–ï¼šå¯¹å€™é€‰å‚æ•°è¿›è¡Œå®Œæ•´è¯„ä¼°

Usage:
    uv run python scripts/search_type_params.py --stage 1  # ç²—æœç´¢
    uv run python scripts/search_type_params.py --stage 2  # ç²¾æœç´¢
    uv run python scripts/search_type_params.py --quick    # å¿«é€Ÿæµ‹è¯•
"""

import sys
import os
import argparse
import numpy as np
import pandas as pd
from itertools import product
from pathlib import Path
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.data_handler.data_loader import DataLoader
from src.config.model_config import ModelConfig
from src.estimation.em_nfxp import run_em_algorithm
from src.estimation.param_search import (
    evaluate_type_separation,
    check_degeneracy,
    suggest_gamma_adjustment,
    evaluate_type_separation_for_low_migration_rate,
    check_degeneracy_for_low_migration_data,
    adaptive_type_identification_score
)
from src.model.likelihood import calculate_log_likelihood


def generate_gamma_grid(n_types: int = 3, granularity: str = 'sparse') -> list:
    """
    ç”Ÿæˆgamma_0å‚æ•°ç½‘æ ¼

    Args:
        n_types: Typeæ•°é‡
        granularity: 'sparse'(ç¨€ç–,~20ç»„åˆ), 'full'(å®Œæ•´,~120ç»„åˆ), 'fine'(ç²¾ç»†,~500ç»„åˆ)

    Returns:
        å‚æ•°ç»„åˆåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(gamma_0_type_0, gamma_0_type_1, gamma_0_type_2)
    """
    if granularity == 'sparse':
        # ç¨€ç–ç½‘æ ¼ï¼šè¦†ç›–å¹¿ä½†ç»„åˆå°‘ (~20ç»„åˆ, ~100åˆ†é’Ÿ)
        base_values = [0.1, 1.0, 5.0]
    elif granularity == 'full':
        # å®Œæ•´ç½‘æ ¼ï¼šåŸå§‹ç²—ç²’åº¦ (~120ç»„åˆ, ~10å°æ—¶)
        base_values = [0.1, 0.5, 1.0, 2.0, 5.0]
    elif granularity == 'fine':
        # ç²¾ç»†ç½‘æ ¼ï¼šå¯†é›†é‡‡æ · (~500ç»„åˆ, ~40å°æ—¶)
        base_values = [0.1, 0.3, 0.5, 0.8, 1.0, 1.5, 2.0, 3.0, 5.0, 8.0]
    else:
        raise ValueError(f"Unknown granularity: {granularity}")

    # ç”Ÿæˆæ‰€æœ‰ç»„åˆ
    combinations = list(product(base_values, repeat=n_types))

    # è¿‡æ»¤æ‰æ²¡æœ‰åŒºåˆ†åº¦çš„ç»„åˆï¼ˆæ‰€æœ‰å€¼ç›¸åŒï¼‰
    combinations = [c for c in combinations if len(set(c)) > 1]

    # è¿‡æ»¤æ‰å·®å¼‚è¿‡å°çš„ç»„åˆ
    combinations = [
        c for c in combinations
        if max(c) / min(c) >= 1.5  # æœ€å¤§å€¼è‡³å°‘æ˜¯æœ€å°å€¼çš„1.5å€
    ]

    print(f"Generated {len(combinations)} parameter combinations ({granularity} granularity)")
    return combinations


def calculate_migration_rate(observed_data: pd.DataFrame) -> float:
    """
    è®¡ç®—æ•°æ®ä¸­çš„è¿ç§»ç‡
    """
    # å‡è®¾å­˜åœ¨migration_flagåˆ—ï¼Œæ ‡è®°æ˜¯å¦å‘ç”Ÿè¿ç§»
    if 'mig_flag' in observed_data.columns:
        n_migrants = observed_data.groupby('individual_id')['mig_flag'].sum().sum()
        total_obs = len(observed_data)
        migration_rate = n_migrants / total_obs if total_obs > 0 else 0.0
    else:
        # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„è¿ç§»æ ‡å¿—ï¼Œå¯ä»¥æ ¹æ®ä½ç½®å˜åŒ–è®¡ç®—
        # è®¡ç®—æ¯ä¸ªä¸ªä½“çš„ä½ç½®å˜åŒ–æ¬¡æ•°
        n_changes = 0
        n_total_obs = 0
        
        for individual_id, group in observed_data.groupby('individual_id'):
            if 'location' in group.columns and len(group) > 1:
                locations = group['location'].values
                changes = sum(1 for i in range(1, len(locations)) if locations[i] != locations[i-1])
                n_changes += changes
                n_total_obs += len(group) - 1
        
        migration_rate = n_changes / n_total_obs if n_total_obs > 0 else 0.0
    
    return migration_rate


def evaluate_single_combination_fast(
    gamma_combination: tuple,
    observed_data: pd.DataFrame,
    state_space: pd.DataFrame,
    transition_matrices: dict,
    beta: float,
    regions_df: pd.DataFrame,
    distance_matrix: np.ndarray,
    adjacency_matrix: np.ndarray,
    verbose: bool = False
) -> dict:
    """
    å¿«é€Ÿè¯„ä¼°å•ä¸ªgamma_0å‚æ•°ç»„åˆï¼ˆåªè¿è¡ŒEæ­¥ï¼‰
    ç°åœ¨æ”¯æŒä½è¿ç§»ç‡æ•°æ®çš„è¯„ä¼°

    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    n_types = len(gamma_combination)
    N = observed_data["individual_id"].nunique()

    # è®¡ç®—è¿ç§»ç‡
    migration_rate = calculate_migration_rate(observed_data)

    # æ„å»ºå‚æ•°å­—å…¸
    params = {
        "alpha_w": 1.0, "lambda": 2.0, "alpha_home": 1.0,
        "rho_base_tier_1": 1.0, "rho_edu": 0.1, "rho_health": 0.1, "rho_house": 0.1,
        **{f"gamma_0_type_{i}": gamma_combination[i] for i in range(n_types)},
        "gamma_1": -0.1, "gamma_2": 0.2, "gamma_3": -0.4,
        "gamma_4": 0.01, "gamma_5": -0.05,
        "alpha_climate": 0.1, "alpha_health": 0.1,
        "alpha_education": 0.1, "alpha_public_services": 0.1,
        "n_choices": 31
    }

    # åˆå§‹åŒ–type probabilitiesä¸ºå‡åŒ€åˆ†å¸ƒ
    pi_k = np.ones(n_types) / n_types

    # è®¡ç®—æ¯ä¸ªtypeçš„log-likelihood
    log_likelihood_matrix = np.zeros((N, n_types))

    for k in range(n_types):
        try:
            # åˆ›å»ºç±»å‹ç‰¹å®šå‚æ•°
            type_specific_params = params.copy()
            if f'gamma_0_type_{k}' in params:
                type_specific_params['gamma_0'] = params[f'gamma_0_type_{k}']
            
            # è°ƒç”¨calculate_log_likelihood with verbose=False
            log_lik_obs = -calculate_log_likelihood(
                params=type_specific_params,
                observed_data=observed_data,
                state_space=state_space,
                agent_type=int(k),
                beta=beta,
                transition_matrices=transition_matrices,
                regions_df=regions_df,
                distance_matrix=distance_matrix,
                adjacency_matrix=adjacency_matrix,
                verbose=False  # å…³é”®ï¼šç¦ç”¨æ‰“å°
            )

            # Sum log-likelihoods for each individual
            observed_data_copy = observed_data.copy()
            observed_data_copy['log_lik_obs'] = log_lik_obs
            individual_log_lik = observed_data_copy.groupby("individual_id")['log_lik_obs'].sum()
            log_likelihood_matrix[:, k] = individual_log_lik.values

        except Exception as e:
            if verbose:
                print(f"  Error computing likelihood for type {k}: {e}")
            log_likelihood_matrix[:, k] = -1e10

    # è®¡ç®—posterior probabilities (Bayes' rule in log space)
    pi_k_safe = np.maximum(pi_k, 1e-10)
    pi_k_safe = pi_k_safe / np.sum(pi_k_safe)

    log_pi_k = np.log(pi_k_safe)
    weighted_log_lik = log_likelihood_matrix + log_pi_k

    # Log-sum-exp trick for numerical stability
    max_log_lik = np.max(weighted_log_lik, axis=1, keepdims=True)
    log_marginal_lik = max_log_lik + np.log(
        np.sum(np.exp(weighted_log_lik - max_log_lik), axis=1, keepdims=True)
    )
    log_posterior = weighted_log_lik - log_marginal_lik

    # Compute posterior probabilities
    posterior_probs = np.exp(log_posterior)
    posterior_probs = np.maximum(posterior_probs, 0)
    row_sums = posterior_probs.sum(axis=1, keepdims=True)
    posterior_probs = posterior_probs / np.maximum(row_sums, 1e-10)

    # è®¡ç®—total log-likelihood
    log_likelihood = np.sum(log_marginal_lik)

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆä½¿ç”¨é€‚åº”ä½è¿ç§»ç‡çš„ç‰ˆæœ¬ï¼‰
    n_params = len(params)
    n_obs = len(observed_data)

    metrics = evaluate_type_separation_for_low_migration_rate(
        posterior_probs, log_likelihood, n_params, n_obs, migration_rate
    )

    # æ·»åŠ gammaå€¼ä¿¡æ¯
    for i, gamma_val in enumerate(gamma_combination):
        metrics[f'gamma_0_type_{i}'] = gamma_val

    # æ£€æŸ¥é€€åŒ–ï¼ˆä½¿ç”¨é€‚åº”ä½è¿ç§»ç‡çš„ç‰ˆæœ¬ï¼‰
    is_degenerate, deg_msg = check_degeneracy_for_low_migration_data(posterior_probs, migration_rate)
    metrics['is_degenerate'] = is_degenerate
    metrics['degeneracy_message'] = deg_msg

    # æ·»åŠ è¿ç§»ç‡ä¿¡æ¯
    metrics['migration_rate'] = migration_rate

    return metrics


def evaluate_single_combination(
    gamma_combination: tuple,
    observed_data: pd.DataFrame,
    state_space: pd.DataFrame,
    transition_matrices: dict,
    beta: float,
    regions_df: pd.DataFrame,
    distance_matrix: np.ndarray,
    adjacency_matrix: np.ndarray,
    n_em_iterations: int = 1,
    verbose: bool = False
) -> dict:
    """
    è¯„ä¼°å•ä¸ªgamma_0å‚æ•°ç»„åˆï¼ˆè¿è¡Œå®Œæ•´EMç®—æ³•ï¼‰
    ç°åœ¨æ”¯æŒä½è¿ç§»ç‡æ•°æ®çš„è¯„ä¼°

    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    n_types = len(gamma_combination)

    # è®¡ç®—è¿ç§»ç‡
    migration_rate = calculate_migration_rate(observed_data)

    # æ„å»ºå‚æ•°å­—å…¸ï¼ˆæ”¯æŒå¤šç»´åº¦ç±»å‹ç‰¹å®šå‚æ•°ï¼‰
    initial_params = {
        "alpha_w": 1.0, "lambda": 2.0, "alpha_home": 1.0,
        "rho_base_tier_1": 1.0, "rho_edu": 0.1, "rho_health": 0.1, "rho_house": 0.1,
        **{f"gamma_0_type_{i}": gamma_combination[i] for i in range(n_types)},
        # æ·»åŠ å…¶ä»–ç±»å‹ç‰¹å®šå‚æ•°
        **{f"gamma_1_type_{i}": -0.1 for i in range(n_types)},  # ç¤ºä¾‹å‚æ•°
        **{f"alpha_home_type_{i}": 1.0 for i in range(n_types)}, # ç¤ºä¾‹å‚æ•°
        **{f"lambda_type_{i}": 2.0 for i in range(n_types)},    # ç¤ºä¾‹å‚æ•°
        "gamma_1": -0.1, "gamma_2": 0.2, "gamma_3": -0.4,
        "gamma_4": 0.01, "gamma_5": -0.05,
        "alpha_climate": 0.1, "alpha_health": 0.1,
        "alpha_education": 0.1, "alpha_public_services": 0.1,
        "n_choices": 31
    }

    if not verbose:
        # ç¦ç”¨æ‰“å°è¾“å‡º
        import io
        import contextlib

        f = io.StringIO()
        with contextlib.redirect_stdout(f):
            result = run_em_algorithm(
                observed_data=observed_data,
                state_space=state_space,
                transition_matrices=transition_matrices,
                beta=beta,
                n_types=n_types,
                regions_df=regions_df,
                distance_matrix=distance_matrix,
                adjacency_matrix=adjacency_matrix,
                max_iterations=n_em_iterations,
                n_choices=31,
                use_migration_behavior_init=True,  # ä½¿ç”¨è¿ç§»è¡Œä¸ºåˆå§‹åŒ–
            )
    else:
        result = run_em_algorithm(
            observed_data=observed_data,
            state_space=state_space,
            transition_matrices=transition_matrices,
            beta=beta,
            n_types=n_types,
            regions_df=regions_df,
            distance_matrix=distance_matrix,
            adjacency_matrix=adjacency_matrix,
            max_iterations=n_em_iterations,
            n_choices=31,
            use_migration_behavior_init=True,  # ä½¿ç”¨è¿ç§»è¡Œä¸ºåˆå§‹åŒ–
        )

    # æå–è¯„ä¼°æŒ‡æ ‡
    posterior_probs = result['posterior_probs']
    log_likelihood = result['final_log_likelihood']
    n_params = len(initial_params)
    n_obs = len(observed_data)

    metrics = evaluate_type_separation_for_low_migration_rate(
        posterior_probs, log_likelihood, n_params, n_obs, migration_rate
    )

    # æ·»åŠ gammaå€¼ä¿¡æ¯
    for i, gamma_val in enumerate(gamma_combination):
        metrics[f'gamma_0_type_{i}'] = gamma_val

    # æ£€æŸ¥é€€åŒ–ï¼ˆä½¿ç”¨é€‚åº”ä½è¿ç§»ç‡çš„ç‰ˆæœ¬ï¼‰
    is_degenerate, deg_msg = check_degeneracy_for_low_migration_data(posterior_probs, migration_rate)
    metrics['is_degenerate'] = is_degenerate
    metrics['degeneracy_message'] = deg_msg

    # æ·»åŠ è¿ç§»ç‡ä¿¡æ¯
    metrics['migration_rate'] = migration_rate

    return metrics


def stage1_coarse_search(
    observed_data: pd.DataFrame,
    state_space: pd.DataFrame,
    transition_matrices: dict,
    beta: float,
    regions_df: pd.DataFrame,
    distance_matrix: np.ndarray,
    adjacency_matrix: np.ndarray,
    n_types: int = 3,
    granularity: str = 'sparse'
) -> pd.DataFrame:
    """
    é˜¶æ®µ1ï¼šç²—ç²’åº¦ç½‘æ ¼æœç´¢
    ç°åœ¨æ”¯æŒä½è¿ç§»ç‡æ•°æ®çš„è¯„ä¼°

    Args:
        granularity: 'sparse'(ç¨€ç–,~20ç»„åˆ), 'full'(å®Œæ•´,~120ç»„åˆ), 'fine'(ç²¾ç»†,~500ç»„åˆ)
    """
    print("="*80)
    print(f"Stage 1: Coarse Grid Search ({granularity} mode)")
    print("="*80)

    # è®¡ç®—è¿ç§»ç‡
    migration_rate = calculate_migration_rate(observed_data)
    print(f"Data migration rate: {migration_rate:.3f}")
    print("="*80)

    # ç”Ÿæˆå‚æ•°ç½‘æ ¼
    gamma_combinations = generate_gamma_grid(n_types, granularity=granularity)

    # å¦‚æœæ˜¯fullæˆ–fineæ¨¡å¼ä¸”ç»„åˆæ•°è¿‡å¤šï¼Œç»™å‡ºè­¦å‘Š
    if len(gamma_combinations) > 50:
        print(f"\nâš ï¸  Warning: {len(gamma_combinations)} combinations detected.")
        print(f"   Estimated time: ~{len(gamma_combinations) * 5 / 60:.1f} hours")
        print(f"   Consider using --granularity sparse for faster search.\n")

    results = []

    for idx, gamma_comb in enumerate(gamma_combinations):
        print(f"\n[{idx+1}/{len(gamma_combinations)}] Testing gamma_0 = {gamma_comb}")

        try:
            # ä½¿ç”¨å¿«é€Ÿè¯„ä¼°å‡½æ•°ï¼ˆåªè¿è¡ŒEæ­¥ï¼‰
            metrics = evaluate_single_combination_fast(
                gamma_comb,
                observed_data, state_space, transition_matrices,
                beta, regions_df, distance_matrix, adjacency_matrix,
                verbose=False
            )
            results.append(metrics)

            # æ‰“å°å…³é”®æŒ‡æ ‡
            print(f"  Balance score: {metrics['balance_score']:.3f}, "
                  f"Type probs: [{metrics['type_0_prob']:.2f}, "
                  f"{metrics['type_1_prob']:.2f}, {metrics['type_2_prob']:.2f}], "
                  f"BIC: {metrics['bic']:.1f}, Migration rate: {migration_rate:.3f}")

        except Exception as e:
            print(f"  Error: {e}")
            continue

    # è½¬ä¸ºDataFrame
    results_df = pd.DataFrame(results)

    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆç»“æœ
    if len(results_df) == 0:
        print("\nâŒ No valid results obtained. All combinations failed.")
        print("   This may indicate:")
        print("   1. Numerical issues with the current parameter ranges")
        print("   2. Data loading problems")
        print("   3. Bellman equation convergence failures")
        return results_df

    # ä¿å­˜ç»“æœ
    output_path = 'results/param_search_stage1.csv'
    results_df.to_csv(output_path, index=False)
    print(f"\nâœ“ Stage 1 results saved to: {output_path}")

    # æ˜¾ç¤ºtop 5
    print("\n" + "="*80)
    print(f"Top 5 Parameter Combinations (by balance score) - Migration rate: {migration_rate:.3f}")
    print("="*80)

    n_results = min(5, len(results_df))
    top5 = results_df.nlargest(n_results, 'balance_score')
    for idx, row in top5.iterrows():
        print(f"\n{idx+1}. gamma_0 = [{row['gamma_0_type_0']:.1f}, "
              f"{row['gamma_0_type_1']:.1f}, {row['gamma_0_type_2']:.1f}]")
        print(f"   Balance score: {row['balance_score']:.3f}")
        print(f"   Type probs: [{row['type_0_prob']:.2f}, "
              f"{row['type_1_prob']:.2f}, {row['type_2_prob']:.2f}]")
        print(f"   BIC: {row['bic']:.1f}, LogLik: {row['log_likelihood']:.1f}")
        print(f"   Status: {row['degeneracy_message']}")

    return results_df


def stage2_fine_optimization(
    results_stage1: pd.DataFrame,
    observed_data: pd.DataFrame,
    state_space: pd.DataFrame,
    transition_matrices: dict,
    beta: float,
    regions_df: pd.DataFrame,
    distance_matrix: np.ndarray,
    adjacency_matrix: np.ndarray,
    top_k: int = 5
) -> pd.DataFrame:
    """
    é˜¶æ®µ2ï¼šå¯¹top candidatesè¿›è¡Œç²¾ç»†è¯„ä¼°
    ç°åœ¨æ”¯æŒä½è¿ç§»ç‡æ•°æ®çš„è¯„ä¼°
    """
    print("\n" + "="*80)
    print("Stage 2: Fine-grained Optimization")
    print("="*80)

    # è®¡ç®—è¿ç§»ç‡
    migration_rate = calculate_migration_rate(observed_data)
    print(f"Data migration rate: {migration_rate:.3f}")
    print("="*80)

    # é€‰æ‹©top Kä¸ªå€™é€‰
    candidates = results_stage1.nlargest(top_k, 'balance_score')

    results = []

    for idx, row in candidates.iterrows():
        gamma_comb = (
            row['gamma_0_type_0'],
            row['gamma_0_type_1'],
            row['gamma_0_type_2']
        )

        print(f"\n[{idx+1}/{len(candidates)}] Full evaluation of gamma_0 = {gamma_comb}")

        try:
            metrics = evaluate_single_combination(
                gamma_comb,
                observed_data, state_space, transition_matrices,
                beta, regions_df, distance_matrix, adjacency_matrix,
                n_em_iterations=5,  # è¿è¡Œ5æ¬¡EMè¿­ä»£
                verbose=True  # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            )
            results.append(metrics)

        except Exception as e:
            print(f"  Error: {e}")
            continue

    results_df = pd.DataFrame(results)

    # ä¿å­˜ç»“æœ
    output_path = 'results/param_search_stage2.csv'
    results_df.to_csv(output_path, index=False)
    print(f"\nâœ“ Stage 2 results saved to: {output_path}")

    # é€‰æ‹©æœ€ä¼˜å‚æ•°
    print("\n" + "="*80)
    print(f"FINAL RECOMMENDATION - Migration rate: {migration_rate:.3f}")
    print("="*80)

    # ç»¼åˆè¯„åˆ†ï¼šbalance_score + BICå½’ä¸€åŒ–
    results_df['composite_score'] = (
        results_df['balance_score'] -
        (results_df['bic'] - results_df['bic'].min()) / (results_df['bic'].max() - results_df['bic'].min()) * 0.3
    )

    best = results_df.loc[results_df['composite_score'].idxmax()]

    print(f"\nğŸ¯ Recommended gamma_0 values:")
    print(f"   gamma_0_type_0 = {best['gamma_0_type_0']:.2f}")
    print(f"   gamma_0_type_1 = {best['gamma_0_type_1']:.2f}")
    print(f"   gamma_0_type_2 = {best['gamma_0_type_2']:.2f}")
    print(f"\nğŸ“Š Performance:")
    print(f"   Balance score: {best['balance_score']:.3f}")
    print(f"   Type distribution: [{best['type_0_prob']:.2f}, "
          f"{best['type_1_prob']:.2f}, {best['type_2_prob']:.2f}]")
    print(f"   BIC: {best['bic']:.1f}")
    print(f"   Migration rate: {migration_rate:.3f}")
    print(f"   Status: {best['degeneracy_message']}")

    # ä¿å­˜æ¨èå‚æ•°åˆ°JSON
    recommendation = {
        'gamma_0_type_0': float(best['gamma_0_type_0']),
        'gamma_0_type_1': float(best['gamma_0_type_1']),
        'gamma_0_type_2': float(best['gamma_0_type_2']),
        'migration_rate': migration_rate,
        'metrics': {
            'balance_score': float(best['balance_score']),
            'type_probs': [float(best['type_0_prob']), float(best['type_1_prob']), float(best['type_2_prob'])],
            'bic': float(best['bic']),
            'log_likelihood': float(best['log_likelihood']),
            'migration_rate': migration_rate
        },
        'timestamp': datetime.now().isoformat()
    }

    with open('results/recommended_gamma_0.json', 'w') as f:
        json.dump(recommendation, f, indent=2)

    print(f"\nâœ“ Recommendation saved to: results/recommended_gamma_0.json")

    return results_df


def quick_test(
    observed_data: pd.DataFrame,
    state_space: pd.DataFrame,
    transition_matrices: dict,
    beta: float,
    regions_df: pd.DataFrame,
    distance_matrix: np.ndarray,
    adjacency_matrix: np.ndarray
):
    """
    å¿«é€Ÿæµ‹è¯•å‡ ä¸ªå…¸å‹ç»„åˆ
    """
    print("="*80)
    print("Quick Test Mode - Extreme Differentiation Strategy")
    print("="*80)

    test_combinations = [
        # åŸå§‹æµ‹è¯•ï¼ˆå·²çŸ¥ä¼šé€€åŒ–ï¼‰
        # (0.5, 2.0, 5.0),   # ä½-ä¸­-é«˜
        # (0.1, 1.0, 10.0),  # æä½-ä¸­-æé«˜
        # (1.0, 3.0, 5.0),   # ä¸­-é«˜-æé«˜
        # (0.3, 1.5, 4.0),   # ä½-ä¸­-é«˜ï¼ˆæ¸©å’Œï¼‰

        # æ–°ç­–ç•¥ï¼šæç«¯å·®å¼‚åŒ–å‚æ•°ï¼ˆ400-1000å€å·®è·ï¼‰
        (0.05, 1.0, 20.0),   # æä½-ä¸­-æé«˜ (400å€)
        (0.1, 2.0, 10.0),    # ä½-ä¸­-é«˜ (100å€)
        (0.01, 0.5, 15.0),   # è¶…ä½-ä¸­ä½-è¶…é«˜ (1500å€)
        (0.02, 1.0, 30.0),   # è¶…ä½-ä¸­-è¶…è¶…é«˜ (1500å€)
    ]

    print("\nTesting extreme parameter differentiation to avoid type degeneracy...")
    print("Hypothesis: Need >100x difference between min and max gamma_0\n")

    for idx, gamma_comb in enumerate(test_combinations, 1):
        ratio = max(gamma_comb) / min(gamma_comb)
        print(f"\n[{idx}/{len(test_combinations)}] Testing gamma_0 = {gamma_comb}")
        print(f"  Differentiation ratio: {ratio:.0f}x")

        metrics = evaluate_single_combination_fast(
            gamma_comb,
            observed_data, state_space, transition_matrices,
            beta, regions_df, distance_matrix, adjacency_matrix,
            verbose=True
        )

        print(f"  âœ“ Balance: {metrics['balance_score']:.3f}, "
              f"Types: [{metrics['type_0_prob']:.2f}, {metrics['type_1_prob']:.2f}, {metrics['type_2_prob']:.2f}]")

        if metrics['balance_score'] > 0.5:
            print(f"  ğŸ¯ SUCCESS! Found balanced distribution!")


def main():
    parser = argparse.ArgumentParser(description='Search for optimal type-specific parameters')
    parser.add_argument('--stage', type=int, choices=[1, 2], help='Search stage (1=coarse, 2=fine)')
    parser.add_argument('--quick', action='store_true', help='Quick test mode')
    parser.add_argument('--granularity', type=str, choices=['sparse', 'full', 'fine'],
                       default='sparse',
                       help='Grid granularity: sparse(~20,1.7h), full(~120,10h), fine(~500,40h)')
    args = parser.parse_args()

    # åŠ è½½æ•°æ®
    print("Loading data...")
    config = ModelConfig()
    data_loader = DataLoader(config)

    distance_matrix = data_loader.load_distance_matrix()
    adjacency_matrix = data_loader.load_adjacency_matrix()
    observed_data, regions_df, state_space, transition_matrices = \
        data_loader.create_estimation_dataset_and_state_space(simplified_state=True)

    beta = config.discount_factor

    print(f"Data loaded: {len(observed_data)} observations, {len(state_space)} states\n")

    # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
    Path('results').mkdir(exist_ok=True)

    if args.quick:
        quick_test(
            observed_data, state_space, transition_matrices,
            beta, regions_df, distance_matrix, adjacency_matrix
        )

    elif args.stage == 1:
        stage1_coarse_search(
            observed_data, state_space, transition_matrices,
            beta, regions_df, distance_matrix, adjacency_matrix,
            granularity=args.granularity
        )

    elif args.stage == 2:
        # åŠ è½½stage 1ç»“æœ
        if not os.path.exists('results/param_search_stage1.csv'):
            print("Error: Stage 1 results not found. Run --stage 1 first.")
            return

        results_stage1 = pd.read_csv('results/param_search_stage1.csv')
        stage2_fine_optimization(
            results_stage1,
            observed_data, state_space, transition_matrices,
            beta, regions_df, distance_matrix, adjacency_matrix
        )

    else:
        # è‡ªåŠ¨è¿è¡Œä¸¤ä¸ªé˜¶æ®µ
        print("Running both stages automatically...\n")

        results_stage1 = stage1_coarse_search(
            observed_data, state_space, transition_matrices,
            beta, regions_df, distance_matrix, adjacency_matrix,
            granularity=args.granularity
        )

        stage2_fine_optimization(
            results_stage1,
            observed_data, state_space, transition_matrices,
            beta, regions_df, distance_matrix, adjacency_matrix
        )


if __name__ == '__main__':
    main()
